import { CloudflareWorkersAIEmbeddings } from "@langchain/cloudflare";
import { CloudflareVectorizeStore } from "@langchain/cloudflare";
import type { messagingApi } from "@line/bot-sdk";

/**
 * Enhanced Background processing function with RAG support
 * Processes user messages using AI with optional RAG context from chat history
 */
export async function processMessageInBackground(
	AI: Ai,
	client: messagingApi.MessagingApiClient,
	targetId: string,
	userMessage: string,
	vectorizeIndex?: VectorizeIndex,
): Promise<void> {
	try {
		console.log("Background processing started for:", userMessage);

		let contextualPrompt = userMessage;
		let ragContext = "";

		// Try to use RAG if Vectorize index is available
		if (vectorizeIndex) {
			try {
				console.log("Attempting RAG-enhanced response...");

				// Initialize embeddings for the user query
				const embeddings = new CloudflareWorkersAIEmbeddings({
					binding: AI,
					modelName: "@cf/baai/bge-m3",
				});

				// Initialize vector store for similarity search
				const vectorStore = new CloudflareVectorizeStore(embeddings, {
					index: vectorizeIndex,
				});

				// Search for relevant documents from stored chat history
				const results = await vectorStore.similaritySearch(userMessage, 3);

				if (results.length > 0) {
					console.log(`Found ${results.length} relevant documents`);

					// Prepare context from search results
					ragContext = results
						.map((doc, index) => {
							return `[é–¢é€£æƒ…å ± ${index + 1}]\n${doc.pageContent}`;
						})
						.join("\n\n");

					// Create enhanced prompt with context
					contextualPrompt = `LINE ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ã“ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

é–¢é€£ã™ã‚‹éå»ã®ä¼šè©±:
${ragContext}

ç¾åœ¨ã®è³ªå•: ${userMessage}

ä¸Šè¨˜ã®é–¢é€£æƒ…å ±ã‚’è¸ã¾ãˆã¦ã€é©åˆ‡ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`;
				} else {
					console.log("No relevant documents found, using general response");
				}
			} catch (ragError) {
				console.error(
					"RAG processing failed, falling back to general response:",
					ragError,
				);
				// Continue with general response if RAG fails
			}
		}

		// Use enhanced LLM for background processing
		const aiResponse = await AI.run("@cf/meta/llama-3.2-3b-instruct", {
			messages: [
				{
					role: "system",
					content: vectorizeIndex
						? "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸLINEãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æ–‡è„ˆã‚’ç†è§£ã—ã€éå»ã®ä¼šè©±å†…å®¹ã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€æ—¥æœ¬èªã§ä¸å¯§ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚é–¢é€£ã™ã‚‹éå»ã®æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ã€ãã‚Œã‚’æ´»ç”¨ã—ã¦å›ç­”ã®è³ªã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚"
						: "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ—¥æœ¬èªã§ä¸å¯§ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
				},
				{
					role: "user",
					content: contextualPrompt,
				},
			],
			max_tokens: 200, // RAGã®å ´åˆã¯ã‚ˆã‚Šè©³ç´°ãªå›ç­”ã‚’è¨±å¯
			temperature: 0.3,
		});

		let responseText = "";
		if (aiResponse && typeof aiResponse === "object") {
			const response = aiResponse as Record<string, unknown>;
			responseText =
				(response.response as string) ||
				(response.result as string) ||
				(response.answer as string) ||
				"";
		}

		if (responseText.trim()) {
			// Prepare enhanced response message
			const messagePrefix = ragContext
				? "ğŸ“š éå»ã®ä¼šè©±ã‚’å‚è€ƒã«ã—ãŸè©³ç´°å›ç­”:"
				: "ğŸ’¡ ã‚ˆã‚Šè©³ã—ã„å›ç­”ã§ã™:";

			// Send follow-up message using Push API
			await client.pushMessage({
				to: targetId,
				messages: [
					{
						type: "text",
						text: `${messagePrefix} ${responseText}`,
					},
				],
			});
			console.log("Enhanced AI response sent successfully");
		}
	} catch (error) {
		console.error("Background processing failed:", error);
		// Don't send error message to user - they already got a fallback response
	}
}
