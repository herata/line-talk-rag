import { CloudflareWorkersAIEmbeddings } from "@langchain/cloudflare";
import { CloudflareVectorizeStore } from "@langchain/cloudflare";
import type { messagingApi } from "@line/bot-sdk";

/**
 * Enhanced Background processing function with RAG support
 * Processes user messages using AI with optional RAG context from chat history
 */
export async function processMessageInBackground(
	AI: Ai,
	client: messagingApi.MessagingApiClient,
	targetId: string,
	userMessage: string,
	vectorizeIndex?: VectorizeIndex,
): Promise<void> {
	try {
		console.log("Background processing started for:", userMessage);

		let contextualPrompt = userMessage;
		let ragContext = "";

		// Try to use RAG if Vectorize index is available
		if (vectorizeIndex) {
			try {
				console.log("Attempting RAG-enhanced response...");

				// Initialize embeddings for the user query
				const embeddings = new CloudflareWorkersAIEmbeddings({
					binding: AI,
					modelName: "@cf/baai/bge-m3",
				});

				// Initialize vector store for similarity search
				const vectorStore = new CloudflareVectorizeStore(embeddings, {
					index: vectorizeIndex,
				});

				// Search for relevant documents from stored chat history (increased count for better context)
				const results = await vectorStore.similaritySearch(userMessage, 5);

				if (results.length > 0) {
					console.log(`Found ${results.length} relevant documents`);

					// Prepare comprehensive context from search results
					ragContext = results
						.map((doc, index) => {
							// Include more metadata for better context
							const metadata = doc.metadata || {};
							const timestamp = metadata.timestamp
								? ` [${metadata.timestamp}]`
								: "";
							const participant = metadata.participant
								? ` (${metadata.participant})`
								: "";

							return `[é–¢é€£æƒ…å ± ${index + 1}]${timestamp}${participant}\n${doc.pageContent}`;
						})
						.join("\n\n");

					// Create detailed prompt with comprehensive context
					contextualPrompt = `ã‚ãªãŸã¯éå»ã®LINEãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å‚ç…§ã§ãã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®é–¢é€£ã™ã‚‹éå»ã®ä¼šè©±å†…å®¹ã‚’è©³ã—ãåˆ†æã—ã¦ã€ç¾åœ¨ã®è³ªå•ã«å¯¾ã™ã‚‹æœ€é©ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€éå»ã®é–¢é€£ä¼šè©±ã€‘:
${ragContext}

ã€ç¾åœ¨ã®è³ªå•ã€‘: ${userMessage}

ã€å›ç­”æŒ‡é‡ã€‘:
1. éå»ã®ä¼šè©±å†…å®¹ã‹ã‚‰é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ç©æ¥µçš„ã«æ´»ç”¨ã—ã¦ãã ã•ã„
2. æ–‡è„ˆã‚„èƒŒæ™¯ã‚’ç†è§£ã—ã¦ã€ã‚ˆã‚Šè©³ç´°ã§çš„ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
3. éå»ã®ä¼šè©±ã§è¨€åŠã•ã‚ŒãŸå†…å®¹ãŒã‚ã‚Œã°ã€ãã‚Œã‚’è¸ã¾ãˆã¦å›ç­”ã—ã¦ãã ã•ã„
4. é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„
5. æ—¥æœ¬èªã§è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„

å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™:`;
				} else {
					console.log("No relevant documents found, using general response");
					contextualPrompt = `ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€ä¸å¯§ã§è©³ç´°ãªæ—¥æœ¬èªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

è³ªå•: ${userMessage}

è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã€é©åˆ‡ã§æœ‰ç”¨ãªå›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚`;
				}
			} catch (ragError) {
				console.error(
					"RAG processing failed, falling back to general response:",
					ragError,
				);
				// Continue with general response if RAG fails
			}
		}

		// Use enhanced LLM for background processing with optimized parameters
		console.log("Calling background AI with enhanced model...");
		const aiResponse = await AI.run("@cf/meta/llama-3.1-8b-instruct", {
			messages: [
				{
					role: "system",
					content: vectorizeIndex
						? "ã‚ãªãŸã¯éå»ã®LINEãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å‚ç…§ã§ãã‚‹è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æä¾›ã•ã‚ŒãŸéå»ã®ä¼šè©±å†…å®¹ã‚’è©³ã—ãåˆ†æã—ã€æ–‡è„ˆã‚’ç†è§£ã—ã¦ã€ç¾åœ¨ã®è³ªå•ã«å¯¾ã™ã‚‹æœ€é©ã§è©³ç´°ãªæ—¥æœ¬èªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚éå»ã®æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ç©æ¥µçš„ã«æ´»ç”¨ã—ã€é–¢é€£æ€§ã‚’æ˜ç¢ºã«ã—ã¦å›ç­”ã®è³ªã‚’é«˜ã‚ã¦ãã ã•ã„ã€‚"
						: "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•è€…ã®æ„å›³ã‚’ç†è§£ã—ã€ä¸å¯§ã§è©³ç´°ã‹ã¤æœ‰ç”¨ãªæ—¥æœ¬èªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
				},
				{
					role: "user",
					content: contextualPrompt,
				},
			],
			max_tokens: 400, // Increased for higher quality model
			temperature: 0.2, // Balanced creativity and consistency
			stream: false,
		});

		console.log(
			"Background AI response received:",
			JSON.stringify(aiResponse, null, 2),
		);

		let responseText = "";

		try {
			if (aiResponse && typeof aiResponse === "object") {
				const response = aiResponse as Record<string, unknown>;
				console.log("Background response keys:", Object.keys(response));

				// Comprehensive response extraction
				if (response.response && typeof response.response === "string") {
					responseText = response.response.trim();
					console.log("Background: Found response in 'response' field");
				} else if (response.result && typeof response.result === "string") {
					responseText = response.result.trim();
					console.log("Background: Found response in 'result' field");
				} else if (response.answer && typeof response.answer === "string") {
					responseText = response.answer.trim();
					console.log("Background: Found response in 'answer' field");
				} else if (response.text && typeof response.text === "string") {
					responseText = response.text.trim();
					console.log("Background: Found response in 'text' field");
				} else {
					// Check nested structures
					const nestedResponse = response.result || response.response;
					if (nestedResponse && typeof nestedResponse === "object") {
						const nested = nestedResponse as Record<string, unknown>;
						console.log("Background: Nested object keys:", Object.keys(nested));

						if (nested.response && typeof nested.response === "string") {
							responseText = nested.response.trim();
							console.log(
								"Background: Found response in nested 'response' field",
							);
						} else if (nested.text && typeof nested.text === "string") {
							responseText = nested.text.trim();
							console.log("Background: Found response in nested 'text' field");
						} else if (nested.content && typeof nested.content === "string") {
							responseText = nested.content.trim();
							console.log(
								"Background: Found response in nested 'content' field",
							);
						}
					}
				}
			}
		} catch (extractError) {
			console.error(
				"Background: Error extracting response text:",
				extractError,
			);
		}

		console.log(`Background: Final extracted response text: "${responseText}"`);
		console.log(`Background: Response text length: ${responseText.length}`);

		if (responseText.trim()) {
			// Enhanced response message with RAG context indicator
			let messagePrefix = "";
			if (ragContext) {
				messagePrefix = "ğŸ“š éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’å‚è€ƒã«ã—ãŸè©³ç´°å›ç­”:\n\n";
			} else {
				messagePrefix = "ğŸ¤– AIå›ç­”:\n\n";
			}

			// Send detailed follow-up message using Push API
			await client.pushMessage({
				to: targetId,
				messages: [
					{
						type: "text",
						text: `${messagePrefix}${responseText}`,
					},
				],
			});
			console.log("Enhanced RAG response sent successfully");
		} else {
			console.error("Background processing produced no response text");
			// Send fallback message
			await client.pushMessage({
				to: targetId,
				messages: [
					{
						type: "text",
						text: "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚ğŸ™",
					},
				],
			});
		}
	} catch (error) {
		console.error("Background processing failed:", error);
		// Don't send error message to user - they already got a fallback response
	}
}
